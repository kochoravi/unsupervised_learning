# -*- coding: utf-8 -*-
"""Unsupervised_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1stZw_5Kse3KZ-tgD2iVM_31I372dtUdL

#Dimensionality_Reduction
"""

# Commented out IPython magic to ensure Python compatibility.
# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

try:
    # %tensorflow_version only exists in Colab.
#     %tensorflow_version 2.x
    IS_COLAB = True
except Exception:
    IS_COLAB = False

# TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ >= "2.0"
import numpy as np

(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train_full = X_train_full.astype(np.float32) / 255
X_test = X_test.astype(np.float32) / 255
X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]

y_test.shape[0]

sample_size_train = 5000
sample_size_test = 3000

indx_train = np.random.choice(X_train.shape[0], sample_size_train)
indx_test=np.random.choice(y_test.shape[0], sample_size_test)

X_train_sample = X_train[indx_train].reshape(sample_size_train,-1)
y_test_sample = y_test[indx_test]

X_test_sample = X_test[indx_test].reshape(sample_size_test, -1)

X_test_sample.shape

from sklearn.manifold import Isomap

reducer = Isomap(n_neighbors=5)
reducer.fit(X_train_sample)

x_trans= reducer.transform(X_test_sample)

import matplotlib.pyplot as plt

len(set(y_test_sample))

fig, ax = plt.subplots()
scatter = ax.scatter(x_trans[:,0], x_trans[:, 1], c = y_test_sample, cmap='Spectral', label= y_test_sample)
legend1 = ax.legend(*scatter.legend_elements(num=len(set(y_test_sample))),
                    loc="upper left", title="Ranking")
ax.add_artist(legend1)
plt.show()

plt.scatter(x_trans[:,0], x_trans[:, 1], c = y_test_sample, edgecolor='none', alpha=0.5,
            cmap=plt.cm.get_cmap('Spectral', 10))
plt.xlabel('component 1')
plt.ylabel('component 2')
plt.colorbar()

reducer.reconstruction_error()





"""#Clustering"""

import time as time
import numpy as np
import matplotlib.pyplot as plt
import mpl_toolkits.mplot3d.axes3d as p3
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_swiss_roll

n_samples = 1500
noise = .01
X, color = make_swiss_roll(n_samples, noise=noise)

fig = plt.figure()
ax = p3.Axes3D(fig)
ax.view_init(7, -80)
ax.scatter(X[:,0], X[:,1], X[:, 2], c=color)

color.dtype

color=plt.cm.jet(color / np.max( color))

fig = plt.figure()
ax = p3.Axes3D(fig)
ax.view_init(7, -80)
ax.scatter(X[:,0], X[:,1], X[:, 2], c=color)

"""###Agglomerative Clustering"""

ag_cls_ward = AgglomerativeClustering(n_clusters=4, linkage='ward')

ag_cls_ward.fit(X)

labels=ag_cls_ward.labels_



labels.astype('float').shape

color = plt.cm.jet(labels.astype('float')/np.max(labels+1))
color.shape

fig = plt.figure()
ax = p3.Axes3D(fig)
ax.view_init(7, -80)
for l in np.unique(cls_labels):
  
  ax.scatter(X[labels==l,0], X[labels==l,1], X[labels==l, 2], color=plt.cm.jet(np.float(l) / np.max(labels + 1)))

ag_cls_ward.children_.shape

"""###scipy hierarchical cluastering:

    https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster
"""

from scipy.cluster.hierarchy import ward, fcluster, dendrogram
from scipy.spatial.distance import pdist

xx = [[i,j] for i in range(4) for j in range(4)  ]
xx=np.array(xx)

#or

X,Y = np.mgrid[0:4:1, 0:4:1]
np.vstack((X.flatten(),Y.flatten())).T

#or 
xy=np.mgrid[0:4:1, 0:4:1].reshape(2,-1).T

xy

plt.scatter(xy[:,0], xy[:, 1])

#pdist: generate pairwise distance matrix -> ward (or single...) 
# generate linkage matrix Z as their output:
#Z This matrix represents a dendrogram, where the first and second elements are the two clusters merged at each step,
#the third element is the distance between these clusters, and the fourth element is the size of the new cluster - the number of original data points included.
z=ward(pdist(xy))
z

fcluster(z, t=3, criterion='distance')

dn = dendrogram(z)

X = [[0, 0], [0, 1], [1, 0],
     [0, 4], [0, 3], [1, 4],
     [4, 0], [3, 0], [4, 1],
     [4, 4], [3, 4], [4, 3]]

z=ward(pdist(X))
dendrogram(z, truncate_mode = 'level', p=2)

z

z.shape

z=ward(pdist(X))
dendrogram(z)



model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)

model = model.fit(X)

model.children_

model.distances_

len(X)

"""###2d"""

n_samples = 1500
noise = 1
X, _ = make_swiss_roll(n_samples, noise=noise)
X2d = X[:, 0::2]

X2d.shape

plt.scatter(X2d[:,0], X2d[:,1])



# Plot result
fig = plt.figure()
ax = p3.Axes3D(fig)
ax.view_init(7, -80)
for l in np.unique(label):
    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],
               color=plt.cm.jet(np.float(l) / np.max(label + 1)),
               s=20, edgecolor='k')